{"metadata":{"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HackLive 3: Guided Hackathon - NLP (Analytics vidhya)","metadata":{}},{"cell_type":"markdown","source":"<h2> Performance metric </h2>\n\n<h4> Micro F1 score </h4>","metadata":{}},{"cell_type":"markdown","source":"## Importing the data and necessary libraries","metadata":{}},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import hstack\nfrom sklearn.metrics import f1_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/training-set/Train.csv')\ndf.drop('id', axis=1, inplace=True) #Unnecesary Features\ndf.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> There are total 14004 research papers(rows) in which Abstract gives us the gist of the research paper, rows such as Computer Science, Mathematics, Physics, Statistics gives us the primary domain of the research paper and the remaining 25 columns are the target columns(labels) </h3>","metadata":{}},{"cell_type":"markdown","source":"<h4> Above is the sample of a Abstract of a research paper </h4>","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv('../input/test-set/Test.csv')\ndf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TARGET_COLS = ['Analysis of PDEs', 'Applications',\n               'Artificial Intelligence', 'Astrophysics of Galaxies',\n               'Computation and Language', 'Computer Vision and Pattern Recognition',\n               'Cosmology and Nongalactic Astrophysics',\n               'Data Structures and Algorithms', 'Differential Geometry',\n               'Earth and Planetary Astrophysics', 'Fluid Dynamics',\n               'Information Theory', 'Instrumentation and Methods for Astrophysics',\n               'Machine Learning', 'Materials Science', 'Methodology', 'Number Theory',\n               'Optimization and Control', 'Representation Theory', 'Robotics',\n               'Social and Information Networks', 'Statistics Theory',\n               'Strongly Correlated Electrons', 'Superconductivity',\n               'Systems and Control']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Text preprocessing </h3>","metadata":{}},{"cell_type":"code","source":"#Could had done by removing stopwords using NLTK\ndef remove_punctuations(x):\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’' + '…':\n        x = x.replace(punct, '')\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is a generalized replacement of misspelled words which i use for all projects so some words here may not be actually in abstract\ndef misspelled_words(x):\n    x = x.replace('colour', 'color').replace('centre', 'center').replace('didnt', 'did not').replace('doesnt', 'does not') \\\n        .replace('isnt', 'is not').replace('shouldnt', 'should not').replace('favourite', 'favorite').replace('travelling', 'traveling') \\\n        .replace('counselling', 'counseling').replace('theatre', 'theater').replace('cancelled', 'canceled').replace('labour', 'labor') \\\n        .replace('organisation', 'organization').replace('wwii', 'world war 2').replace('citicise', 'criticize') \\\n        .replace('instagram', 'social medium').replace('whatsapp', 'social medium').replace('WeChat', 'social medium') \\\n        .replace('snapchat', 'social medium').replace('Snapchat', 'social medium').replace('btech', 'B.Tech').replace('Quorans', 'Quora') \\\n        .replace('cryptocurrency', 'crypto currency').replace('cryptocurrencies', 'crypto currency').replace('behaviour', 'behavior') \\\n        .replace('analyse', 'analyze').replace('licence', 'license').replace('programme', 'program').replace('grey', 'gray') \\\n        .replace('realise', 'realize').replace('bcom', 'B.Com').replace('defence', 'defense').replace('mtech', 'M.Tech') \\\n        .replace('Btech', 'B.Tech').replace('honours', 'honors').replace('recognise', 'recognize').replace('programr', 'programmer') \\\n        .replace('programrs', 'programmer').replace('hasnt', 'has not').replace('litre', 'liter').replace('Isnt', 'is not') \\\n        .replace('learnt', 'learn').replace('favour', 'favor').replace('neighbour', 'neighbor').replace('demonetisation', 'demonetization') \\\n        .replace('₹', '').replace('&', 'and')\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"ABSTRACT\"] = df[\"ABSTRACT\"].apply(lambda x: remove_punctuations(x))\ndf[\"ABSTRACT\"] = df[\"ABSTRACT\"].apply(lambda x: clean_numbers(x))\ndf[\"ABSTRACT\"] = df[\"ABSTRACT\"].apply(lambda x: misspelled_words(x))\ndf_test[\"ABSTRACT\"] = df_test[\"ABSTRACT\"].apply(lambda x: remove_punctuations(x))\ndf_test[\"ABSTRACT\"] = df_test[\"ABSTRACT\"].apply(lambda x: clean_numbers(x))\ndf_test[\"ABSTRACT\"] = df_test[\"ABSTRACT\"].apply(lambda x: misspelled_words(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Splitiing the data into train and validation (80:20) </h3>","metadata":{}},{"cell_type":"code","source":"#Training and Cross-Validation Set\ntrain, val = train_test_split(df, test_size=0.2, random_state=0)\ntrain.shape, val.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Vectorizing train, validation and test dataset using Tfidf vectorizer</h3>","metadata":{}},{"cell_type":"code","source":"tfidfvec = TfidfVectorizer(min_df=3, max_features=None, ngram_range=(1, 2), strip_accents='unicode', stop_words='english')\ntfidfvec.fit(df['ABSTRACT'])\ntrain_vec = tfidfvec.transform(train['ABSTRACT'])\nval_vec = tfidfvec.transform(val['ABSTRACT'])\ntest_vec = tfidfvec.transform(df_test['ABSTRACT'])\ntrain_vec.shape, val_vec.shape, test_vec.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h5> Here after vectorizing we are stacking the remaining 4 features into csr format. Here if we use numpy array format instead of csr format then our RAM won't be able to suffice hence it is important to pass data to our model in csr format </h5>","metadata":{}},{"cell_type":"code","source":"train_data = hstack((train_vec, train[['Computer Science', 'Mathematics', 'Physics', 'Statistics']]), format=\"csr\", dtype='float64')\nval_data = hstack((val_vec, val[['Computer Science', 'Mathematics', 'Physics', 'Statistics']]), format=\"csr\", dtype='float64')\ntest_data = hstack((test_vec, df_test[['Computer Science', 'Mathematics', 'Physics', 'Statistics']]), format=\"csr\", dtype='float64')\ntrain_data.shape, val_data.shape, test_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Using Grid search to find best hyperparameters </h3>\n<h5> Note: Since there was only single hyperparameter to tune hence i used GridSearchCV. If there are more hyperparameters it is wise to choose RandomizedSearchCV </h5>","metadata":{}},{"cell_type":"code","source":"parameters = {\n    'estimator__C': [10 ** x for x in range(-2, 3)]\n}\n\nestimator = OneVsRestClassifier(LogisticRegression(max_iter=500, n_jobs=-1))\nmodel = GridSearchCV(estimator, parameters, scoring='f1_micro', cv=5, n_jobs=-1, refit=False)\nmodel.fit(train_data, train[TARGET_COLS])\nbest_C = model.best_params_['estimator__C']\nprint('The best value of C is', best_C)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Applying ML model using best hyperparameter and predicting on validation data","metadata":{}},{"cell_type":"code","source":"clf = OneVsRestClassifier(LogisticRegression(C = best_C, max_iter=500, n_jobs=-1))\nclf.fit(train_data, train[TARGET_COLS])\npred = clf.predict(val_data)\nf1_score(val[TARGET_COLS], pred, average='micro')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This is a simple hack which is used to find the optimal treshold to calculate the best F1 score\ndef get_best_thresholds(true, preds):\n    thresholds = [i/100 for i in range(100)]\n    best_thresholds = []\n    for idx in range(25):\n        f1_scores = [f1_score(true[:, idx], (preds[:, idx] > thresh) * 1) for thresh in thresholds]\n        best_thresh = thresholds[np.argmax(f1_scores)]\n        best_thresholds.append(best_thresh)\n    return best_thresholds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_preds = clf.predict_proba(val_data)\nbest_thresholds = get_best_thresholds(val[TARGET_COLS].values, val_preds)\nfor i, thresh in enumerate(best_thresholds):\n    val_preds[:, i] = (val_preds[:, i] > thresh) * 1\nf1_score(val[TARGET_COLS], val_preds, average='micro')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> As you can see above the F1 score after finding optimal tresholds has drastically improved from 0.73 to 0.78.\nSuch improvements can lead to the creation of more real life based ML Models","metadata":{}},{"cell_type":"markdown","source":"<h3> Submitting the predictions </h3>","metadata":{}},{"cell_type":"code","source":"ss = pd.read_csv('../input/topic-modeling-for-research-articles-20/submission.csv')\npreds_test = clf.predict_proba(test_data)\n\nfor i, thresh in enumerate(best_thresholds):\n    preds_test[:, i] = (preds_test[:, i] > thresh) * 1\n\nss[TARGET_COLS] = preds_test\nss.to_csv('submission_hacklive_nlp', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}